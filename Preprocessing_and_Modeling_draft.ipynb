{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5f4fd8-ca62-485b-9b93-b4b368690421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://docs.gpt4all.io/gpt4all_python/home.html using the docs as a guide\n",
    "\n",
    "!pip install gpt4all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb2c9c0b-049f-491a-9a02-e20791241d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from gpt4all import GPT4All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f933e0d3-d3e6-4c08-979c-be538975c149",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|████████████████████████| 4.66G/4.66G [01:45<00:00, 44.2MiB/s]\n",
      "Verifying: 100%|███████████████████████████| 4.66G/4.66G [00:07<00:00, 613MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Large Language Models (LLMs) are powerful AI models that require significant computational resources to train and use. Running them efficiently on your laptop requires a combination of hardware, software, and technique optimizations. Here's a comprehensive guide to help you get the most out of your LLMs:\n",
      "\n",
      "**Hardware Optimizations**\n",
      "\n",
      "1. **CPU**: Choose a CPU with multiple cores (at least 4-6) for parallel processing.\n",
      "2. **GPU**: If possible, use a laptop with a dedicated NVIDIA or AMD GPU, as they can significantly accelerate computations.\n",
      "3. **RAM**: Ensure you have at least 16 GB of RAM to accommodate the model's memory requirements.\n",
      "\n",
      "**Software Optimizations**\n",
      "\n",
      "1. **Python version**: Use Python 3.x (e.g., 3.8) for better performance and compatibility with popular libraries like TensorFlow, PyTorch, or JAX.\n",
      "2. **Libraries**: Choose optimized libraries:\n",
      "\t* For CPU-based computations: NumPy, SciPy, and scikit-learn are efficient choices.\n",
      "\t* For GPU-accelerated computations: cuDNN (for NVIDIA GPUs) or ROCm (for AMD GPUs) can provide significant speedups.\n",
      "3. **LLM frameworks**: Utilize optimized LLM frameworks like:\n",
      "\t+ Hugging Face's Transformers library for pre-trained models and fine-tuning capabilities.\n",
      "\t+ PyTorch-Transformers for integrating with the PyTorch ecosystem.\n",
      "\n",
      "**Technique Optimizations**\n",
      "\n",
      "1. **Model pruning**: Reduce model size by removing redundant or less important weights, which can speed up computations.\n",
      "2. **Knowledge distillation**: Train a smaller student model on top of a pre-trained teacher model to transfer knowledge and reduce computational requirements.\n",
      "3. **Batching**: Process data in batches rather than individually to take advantage of parallel processing capabilities.\n",
      "4. **Gradient checkpointing**: Store intermediate gradients during backpropagation, reducing the number of computations required for each iteration.\n",
      "5. **Mixed precision training**: Use lower-precision floating-point numbers (e.g., float16) when possible to reduce memory usage and accelerate computations.\n",
      "\n",
      "**Additional Tips**\n",
      "\n",
      "1. **Use cloud services**: Consider using cloud-based platforms like Google Colab, AWS SageMaker, or Azure Machine Learning Studio for access to powerful hardware and scalability.\n",
      "2. **Optimize your code**: Profile and optimize your Python code using tools like cProfile, line_profiler, or PyCharm's built-in profiler.\n",
      "3. **Monitor system resources**: Keep an eye on CPU usage, memory consumption, and disk space to ensure your laptop can handle the demands of running LLMs.\n",
      "\n",
      "By applying these optimizations, you'll be able to run LLMs more efficiently on your laptop, even with limited hardware resources. Happy modeling!\n"
     ]
    }
   ],
   "source": [
    "model = GPT4All(\"Meta-Llama-3-8B-Instruct.Q4_0.gguf\") # downloads / loads a 4.66GB LLM\n",
    "with model.chat_session():\n",
    "    print(model.generate(\"How can I run LLMs efficiently on my laptop?\", max_tokens=1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea360c35-3566-4a12-bb29-7d10927867d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To fine-tune a Large Language Model (LLM) like BERT, RoBERTa, or XLNet on your local machine, you'll need to use a library that supports training transformer-based models. Here's a step-by-step guide:\n",
      "\n",
      "**Prerequisites:**\n",
      "\n",
      "1. **Python**: You should have Python 3.x installed.\n",
      "2. **PyTorch**: Install PyTorch (version >= 1.9) using pip: `pip install torch`\n",
      "3. **Transformers library**: Install the Transformers library (version >= 4.10) using pip: `pip install transformers`\n",
      "\n",
      "**Step-by-Step Guide:**\n",
      "\n",
      "### 1. Prepare your dataset\n",
      "\n",
      "* Convert your text data into a format that can be used for training, such as JSON or CSV.\n",
      "* Split your data into training (~80%), validation (~15%), and testing sets (5%).\n",
      "\n",
      "### 2. Choose an LLM architecture\n",
      "\n",
      "Select the pre-trained model you want to fine-tune from the Transformers library:\n",
      "```python\n",
      "import torch\n",
      "from transformers import BertTokenizer, BertForSequenceClassification\n",
      "\n",
      "# Load a pre-trained BERT model for sequence classification\n",
      "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
      "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=8)\n",
      "```\n",
      "### 3. Prepare your data for training\n",
      "\n",
      "* Tokenize your text data using the `tokenizer`:\n",
      "```python\n",
      "input_ids = []\n",
      "attention_masks = []\n",
      "\n",
      "for text in your_data:\n",
      "    encoded_dict = tokenizer.encode_plus(\n",
      "        text,\n",
      "        add_special_tokens=True,\n",
      "        max_length=512,\n",
      "        return_attention_mask=True,\n",
      "        padding='max_length',\n",
      "        truncation=True,\n",
      "        return_tensors='pt'\n",
      "    )\n",
      "    input_ids.append(encoded_dict['input_ids'].flatten())\n",
      "    attention_masks.append(encoded_dict['attention_mask'].flatten())\n",
      "\n",
      "# Convert lists to tensors\n",
      "input_ids = torch.tensor(input_ids)\n",
      "attention_masks = torch.tensor(attention_masks)\n",
      "```\n",
      "### 4. Define your training loop\n",
      "\n",
      "Create a PyTorch `DataLoader` for your dataset and define the training loop:\n",
      "```python\n",
      "from torch.utils.data import DataLoader, TensorDataset\n",
      "\n",
      "# Create a tensor dataset from input IDs and attention masks\n",
      "dataset = TensorDataset(input_ids, attention_masks)\n",
      "\n",
      "# Set up data loader with batch size 32\n",
      "batch_size = 32\n",
      "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
      "\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "model.to(device)\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
      "\n",
      "for epoch in range(3):\n",
      "    model.train()\n",
      "    total_loss = 0\n",
      "    for batch in data_loader:\n",
      "        input_ids, attention_mask = map(lambda x: x.to(device), batch)\n",
      "        labels = ...  # your labels\n",
      "\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
      "        loss = criterion(outputs, labels)\n",
      "\n",
      "        loss.backward()\n",
      "        optimizer.step()\n",
      "\n",
      "        total_loss += loss.item()\n",
      "\n",
      "    print(f'Epoch {epoch+1}, Loss: {total_loss / len(data_loader)}')\n",
      "```\n",
      "### 5. Train your model\n",
      "\n",
      "Run the training loop for a specified number of epochs.\n",
      "\n",
      "**Tips and Variations**\n",
      "\n",
      "* Use `torch.nn.DataParallel` to train on multiple GPUs.\n",
      "* Experiment with different hyperparameters, such as learning rate, batch size, and number of epochs.\n",
      "* Try using other LLM architectures or pre-trained models from the Transformers library.\n",
      "* For classification tasks, you may need to modify the model's output layer (e.g., change `num_labels` in `BertForSequenceClassification`) based on your specific task.\n",
      "\n",
      "Remember that fine-tuning a large language model requires significant computational resources and time. Make sure you have enough memory and processing power before starting the training process.\n"
     ]
    }
   ],
   "source": [
    "with model.chat_session():\n",
    "    print(model.generate(\"How can I run train this LLM on my dataset locally?\", max_tokens=1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "15c89106-96a7-497e-8c9f-5661475f0e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What a great question!\n",
      "\n",
      "To build a book recommendation system using a Large Language Model (LLM) like OpenAI's Codex or Google's BERT, you'll need to follow these general steps:\n",
      "\n",
      "1. **Preprocess your data**: Prepare your dataset of books and their corresponding features (e.g., genres, authors, summaries). You can use libraries like Pandas for data manipulation.\n",
      "2. **Train a model on the preprocessed data**: Use the LLM's API or a library like Hugging Face Transformers to train a model that takes in book metadata as input and predicts relevant books based on user preferences (e.g., genre, author).\n",
      "3. **Create an inference pipeline**: Set up a system that can take in new user inputs (e.g., favorite authors, genres) and use the trained model to generate personalized book recommendations.\n",
      "4. **Integrate with your dataframe**: Use the recommended books from step 3 to create a recommendation list for each user in your original dataset.\n",
      "\n",
      "Here's a high-level example of how you can implement this using Python:\n",
      "\n",
      "**Step 1: Preprocess data**\n",
      "\n",
      "```python\n",
      "import pandas as pd\n",
      "\n",
      "# Load your book dataset (e.g., CSV file)\n",
      "df = pd.read_csv('books.csv')\n",
      "\n",
      "# Define the features to use for training (e.g., genres, authors, summaries)\n",
      "features = ['genre', 'author', 'summary']\n",
      "\n",
      "# Create a new dataframe with only these features\n",
      "book_features_df = df[features]\n",
      "```\n",
      "\n",
      "**Step 2: Train a model**\n",
      "\n",
      "```python\n",
      "import torch\n",
      "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
      "\n",
      "# Load the pre-trained LLM (e.g., BERT)\n",
      "model_name = 'bert-base-uncased'\n",
      "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
      "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
      "\n",
      "# Train the model on your book features dataframe\n",
      "train_dataset = ...  # create a dataset from `book_features_df`\n",
      "trainer = ...\n",
      "trainer.train(model, train_dataset)\n",
      "```\n",
      "\n",
      "**Step 3: Create an inference pipeline**\n",
      "\n",
      "```python\n",
      "def get_recommendations(user_input):\n",
      "    \"\"\"\n",
      "    Take in user input (e.g., favorite authors) and return recommended books.\n",
      "    \"\"\"\n",
      "    # Convert user input to a format the model can understand\n",
      "    user_features = ...  # e.g., tokenize text, create embeddings\n",
      "\n",
      "    # Use the trained model to generate recommendations\n",
      "    outputs = model(user_features)\n",
      "    predicted_books = [book for book in df.index if outputs[book] > threshold]\n",
      "\n",
      "    return predicted_books\n",
      "```\n",
      "\n",
      "**Step 4: Integrate with your dataframe**\n",
      "\n",
      "```python\n",
      "# Load your original dataset (e.g., CSV file)\n",
      "user_data_df = pd.read_csv('users.csv')\n",
      "\n",
      "# Create a new column to store recommended books for each user\n",
      "recommended_books_df = user_data_df.copy()\n",
      "recommended_books_df['recommended_books'] = None\n",
      "\n",
      "for index, row in user_data_df.iterrows():\n",
      "    # Get the user's input (e.g., favorite authors)\n",
      "    user_input = ...  # e.g., extract text from a column or prompt\n",
      "\n",
      "    # Generate recommendations using the inference pipeline\n",
      "    recommended_books = get_recommendations(user_input)\n",
      "\n",
      "    # Store the recommended books for this user in the new dataframe\n",
      "    recommended_books_df.loc[index, 'recommended_books'] = ', '.join(recommended_books)\n",
      "```\n",
      "\n",
      "This is just a rough outline of how you can integrate an LLM with your book dataset to build a recommendation system. You'll need to modify and extend these steps based on your specific requirements and data.\n",
      "\n",
      "Remember to explore the documentation for the LLM's API or library, as well as any relevant tutorials or examples, to learn more about using them in your project.\n"
     ]
    }
   ],
   "source": [
    "with model.chat_session():\n",
    "    print(model.generate(\"How can I run use this LLM and my dataframe to make a book recommendation system?\", max_tokens=1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20dcd340-af6e-414d-a24c-7723c5f3f780",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/cleaned_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4dbea56e-0d77-48d2-9c42-0642c35eccd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>isbn13</th>\n",
       "      <th>isbn10</th>\n",
       "      <th>title</th>\n",
       "      <th>subtitle</th>\n",
       "      <th>authors</th>\n",
       "      <th>categories</th>\n",
       "      <th>thumbnail</th>\n",
       "      <th>description</th>\n",
       "      <th>published_year</th>\n",
       "      <th>average_rating</th>\n",
       "      <th>num_pages</th>\n",
       "      <th>ratings_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9780002261982</td>\n",
       "      <td>0002261987</td>\n",
       "      <td>Spider's Web</td>\n",
       "      <td>A Novel</td>\n",
       "      <td>Charles Osborne;Agatha Christie</td>\n",
       "      <td>Detective and mystery stories</td>\n",
       "      <td>http://books.google.com/books/content?id=gA5GP...</td>\n",
       "      <td>A new 'Christie for Christmas' -- a full-lengt...</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>3.83</td>\n",
       "      <td>241.0</td>\n",
       "      <td>5164.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9780006380832</td>\n",
       "      <td>0006380832</td>\n",
       "      <td>Empires of the Monsoon</td>\n",
       "      <td>A History of the Indian Ocean and Its Invaders</td>\n",
       "      <td>Richard Hall</td>\n",
       "      <td>Africa, East</td>\n",
       "      <td>http://books.google.com/books/content?id=MuPEQ...</td>\n",
       "      <td>Until Vasco da Gama discovered the sea-route t...</td>\n",
       "      <td>1998.0</td>\n",
       "      <td>4.41</td>\n",
       "      <td>608.0</td>\n",
       "      <td>65.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9780006470229</td>\n",
       "      <td>000647022X</td>\n",
       "      <td>The Gap Into Madness</td>\n",
       "      <td>Chaos and Order</td>\n",
       "      <td>Stephen R. Donaldson</td>\n",
       "      <td>Hyland, Morn (Fictitious character)</td>\n",
       "      <td>http://books.google.com/books/content?id=4oXav...</td>\n",
       "      <td>A new-cover reissue of the fourth book in the ...</td>\n",
       "      <td>1994.0</td>\n",
       "      <td>4.15</td>\n",
       "      <td>743.0</td>\n",
       "      <td>103.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9780006499626</td>\n",
       "      <td>0006499627</td>\n",
       "      <td>Miss Marple</td>\n",
       "      <td>The Complete Short Stories</td>\n",
       "      <td>Agatha Christie</td>\n",
       "      <td>Detective and mystery stories, English</td>\n",
       "      <td>http://books.google.com/books/content?id=a96qP...</td>\n",
       "      <td>Miss Marple featured in 20 short stories, publ...</td>\n",
       "      <td>1997.0</td>\n",
       "      <td>4.20</td>\n",
       "      <td>359.0</td>\n",
       "      <td>6235.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9780006551812</td>\n",
       "      <td>0006551815</td>\n",
       "      <td>'Tis</td>\n",
       "      <td>A Memoir</td>\n",
       "      <td>Frank McCourt</td>\n",
       "      <td>Ireland</td>\n",
       "      <td>http://books.google.com/books/content?id=Q3BhQ...</td>\n",
       "      <td>FROM THE PULIZER PRIZE-WINNING AUTHOR OF THE #...</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>3.68</td>\n",
       "      <td>495.0</td>\n",
       "      <td>44179.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          isbn13      isbn10                   title  \\\n",
       "0  9780002261982  0002261987            Spider's Web   \n",
       "1  9780006380832  0006380832  Empires of the Monsoon   \n",
       "2  9780006470229  000647022X    The Gap Into Madness   \n",
       "3  9780006499626  0006499627             Miss Marple   \n",
       "4  9780006551812  0006551815                    'Tis   \n",
       "\n",
       "                                         subtitle  \\\n",
       "0                                         A Novel   \n",
       "1  A History of the Indian Ocean and Its Invaders   \n",
       "2                                 Chaos and Order   \n",
       "3                      The Complete Short Stories   \n",
       "4                                        A Memoir   \n",
       "\n",
       "                           authors                              categories  \\\n",
       "0  Charles Osborne;Agatha Christie           Detective and mystery stories   \n",
       "1                     Richard Hall                            Africa, East   \n",
       "2             Stephen R. Donaldson     Hyland, Morn (Fictitious character)   \n",
       "3                  Agatha Christie  Detective and mystery stories, English   \n",
       "4                    Frank McCourt                                 Ireland   \n",
       "\n",
       "                                           thumbnail  \\\n",
       "0  http://books.google.com/books/content?id=gA5GP...   \n",
       "1  http://books.google.com/books/content?id=MuPEQ...   \n",
       "2  http://books.google.com/books/content?id=4oXav...   \n",
       "3  http://books.google.com/books/content?id=a96qP...   \n",
       "4  http://books.google.com/books/content?id=Q3BhQ...   \n",
       "\n",
       "                                         description  published_year  \\\n",
       "0  A new 'Christie for Christmas' -- a full-lengt...          2000.0   \n",
       "1  Until Vasco da Gama discovered the sea-route t...          1998.0   \n",
       "2  A new-cover reissue of the fourth book in the ...          1994.0   \n",
       "3  Miss Marple featured in 20 short stories, publ...          1997.0   \n",
       "4  FROM THE PULIZER PRIZE-WINNING AUTHOR OF THE #...          2000.0   \n",
       "\n",
       "   average_rating  num_pages  ratings_count  \n",
       "0            3.83      241.0         5164.0  \n",
       "1            4.41      608.0           65.0  \n",
       "2            4.15      743.0          103.0  \n",
       "3            4.20      359.0         6235.0  \n",
       "4            3.68      495.0        44179.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e785c1f5-b0aa-4cfe-8e31-5bb647138076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Step 1: Preprocess data**\n",
    "\n",
    "features = ['title', 'subtitle', 'categories', 'description', 'authors', 'average_rating', 'ratings_count']\n",
    "features_df = df[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8df9b386-a1d3-40be-a3df-ee645ea28518",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>subtitle</th>\n",
       "      <th>categories</th>\n",
       "      <th>description</th>\n",
       "      <th>authors</th>\n",
       "      <th>average_rating</th>\n",
       "      <th>ratings_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Spider's Web</td>\n",
       "      <td>A Novel</td>\n",
       "      <td>Detective and mystery stories</td>\n",
       "      <td>A new 'Christie for Christmas' -- a full-lengt...</td>\n",
       "      <td>Charles Osborne;Agatha Christie</td>\n",
       "      <td>3.83</td>\n",
       "      <td>5164.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Empires of the Monsoon</td>\n",
       "      <td>A History of the Indian Ocean and Its Invaders</td>\n",
       "      <td>Africa, East</td>\n",
       "      <td>Until Vasco da Gama discovered the sea-route t...</td>\n",
       "      <td>Richard Hall</td>\n",
       "      <td>4.41</td>\n",
       "      <td>65.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Gap Into Madness</td>\n",
       "      <td>Chaos and Order</td>\n",
       "      <td>Hyland, Morn (Fictitious character)</td>\n",
       "      <td>A new-cover reissue of the fourth book in the ...</td>\n",
       "      <td>Stephen R. Donaldson</td>\n",
       "      <td>4.15</td>\n",
       "      <td>103.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Miss Marple</td>\n",
       "      <td>The Complete Short Stories</td>\n",
       "      <td>Detective and mystery stories, English</td>\n",
       "      <td>Miss Marple featured in 20 short stories, publ...</td>\n",
       "      <td>Agatha Christie</td>\n",
       "      <td>4.20</td>\n",
       "      <td>6235.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>'Tis</td>\n",
       "      <td>A Memoir</td>\n",
       "      <td>Ireland</td>\n",
       "      <td>FROM THE PULIZER PRIZE-WINNING AUTHOR OF THE #...</td>\n",
       "      <td>Frank McCourt</td>\n",
       "      <td>3.68</td>\n",
       "      <td>44179.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    title                                        subtitle  \\\n",
       "0            Spider's Web                                         A Novel   \n",
       "1  Empires of the Monsoon  A History of the Indian Ocean and Its Invaders   \n",
       "2    The Gap Into Madness                                 Chaos and Order   \n",
       "3             Miss Marple                      The Complete Short Stories   \n",
       "4                    'Tis                                        A Memoir   \n",
       "\n",
       "                               categories  \\\n",
       "0           Detective and mystery stories   \n",
       "1                            Africa, East   \n",
       "2     Hyland, Morn (Fictitious character)   \n",
       "3  Detective and mystery stories, English   \n",
       "4                                 Ireland   \n",
       "\n",
       "                                         description  \\\n",
       "0  A new 'Christie for Christmas' -- a full-lengt...   \n",
       "1  Until Vasco da Gama discovered the sea-route t...   \n",
       "2  A new-cover reissue of the fourth book in the ...   \n",
       "3  Miss Marple featured in 20 short stories, publ...   \n",
       "4  FROM THE PULIZER PRIZE-WINNING AUTHOR OF THE #...   \n",
       "\n",
       "                           authors  average_rating  ratings_count  \n",
       "0  Charles Osborne;Agatha Christie            3.83         5164.0  \n",
       "1                     Richard Hall            4.41           65.0  \n",
       "2             Stephen R. Donaldson            4.15          103.0  \n",
       "3                  Agatha Christie            4.20         6235.0  \n",
       "4                    Frank McCourt            3.68        44179.0  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b1f618c7-bc77-4bbb-b365-a24a167f24f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1745.6000000000001"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.8*len(features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70661cf8-7021-4b3d-9a1d-dc421ddc879a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a51b1dfb-7a0b-4d94-aa66-64ceac312149",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.trainer because of the following error (look up to see its traceback):\nFailed to import transformers.integrations.integration_utils because of the following error (look up to see its traceback):\nFailed to import transformers.modeling_tf_utils because of the following error (look up to see its traceback):\nYour currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/activations_tf.py:22\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 22\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mImportError\u001b[39;00m):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tf_keras'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/utils/import_utils.py:1817\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1816\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1817\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m   1818\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _bootstrap\u001b[38;5;241m.\u001b[39m_gcd_import(name[level:], package, level)\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1204\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1176\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1147\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:690\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:940\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/modeling_tf_utils.py:38\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataCollatorWithPadding, DefaultDataCollator\n\u001b[0;32m---> 38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations_tf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_tf_activation\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfiguration_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PretrainedConfig\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/activations_tf.py:27\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m parse(keras\u001b[38;5;241m.\u001b[39m__version__)\u001b[38;5;241m.\u001b[39mmajor \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     28\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYour currently installed version of Keras is Keras 3, but this is not yet supported in \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     29\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTransformers. Please install the backwards-compatible tf-keras package with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     30\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`pip install tf-keras`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     31\u001b[0m         )\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_gelu\u001b[39m(x):\n",
      "\u001b[0;31mValueError\u001b[0m: Your currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/utils/import_utils.py:1817\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1816\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1817\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m   1818\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _bootstrap\u001b[38;5;241m.\u001b[39m_gcd_import(name[level:], package, level)\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1204\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1176\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1147\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:690\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:940\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/integrations/integration_utils.py:36\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpackaging\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PreTrainedModel, TFPreTrainedModel\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__ \u001b[38;5;28;01mas\u001b[39;00m version\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1229\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/utils/import_utils.py:1805\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1804\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m-> 1805\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module[name])\n\u001b[1;32m   1806\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/utils/import_utils.py:1819\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1818\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1819\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1820\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1821\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1822\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.modeling_tf_utils because of the following error (look up to see its traceback):\nYour currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/utils/import_utils.py:1817\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1816\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1817\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m   1818\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _bootstrap\u001b[38;5;241m.\u001b[39m_gcd_import(name[level:], package, level)\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1204\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1176\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1147\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:690\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:940\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/trainer.py:42\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Integrations must be imported before ML frameworks:\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# isort: off\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mintegrations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     43\u001b[0m     get_reporting_integration_callbacks,\n\u001b[1;32m     44\u001b[0m     hp_params,\n\u001b[1;32m     45\u001b[0m )\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# isort: on\u001b[39;00m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1229\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/utils/import_utils.py:1805\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1804\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m-> 1805\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module[name])\n\u001b[1;32m   1806\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/utils/import_utils.py:1819\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1818\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1819\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1820\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1821\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1822\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.integrations.integration_utils because of the following error (look up to see its traceback):\nFailed to import transformers.modeling_tf_utils because of the following error (look up to see its traceback):\nYour currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModelForSequenceClassification, AutoTokenizer\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Trainer\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1229\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/utils/import_utils.py:1805\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1803\u001b[0m     value \u001b[38;5;241m=\u001b[39m Placeholder\n\u001b[1;32m   1804\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m-> 1805\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module[name])\n\u001b[1;32m   1806\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[1;32m   1807\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/utils/import_utils.py:1819\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1817\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m   1818\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1819\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1820\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1821\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1822\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.trainer because of the following error (look up to see its traceback):\nFailed to import transformers.integrations.integration_utils because of the following error (look up to see its traceback):\nFailed to import transformers.modeling_tf_utils because of the following error (look up to see its traceback):\nYour currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`."
     ]
    }
   ],
   "source": [
    "# **Step 2: Train a model**\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from transformers import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "be8acc74-4c63-47d6-8320-6bfd36267500",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5b48742169b4acd8177698c70d76e8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a19a829bb474e8c8d73b13e013c6d7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd671b224bc140d59bbaa1185ff103aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e62eb2a64b2642e8b85226055270ce8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "142e016402c844e39d3b9472453ce784",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ebc90efd-b90e-4868-8358-f792ed8b1cbf",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Train the model on your book features dataframe\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# create a dataset from `features_df` - about 80% of len(features_df)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m features_df\u001b[38;5;241m.\u001b[39msample(\u001b[38;5;241m1745\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(model\u001b[38;5;241m=\u001b[39mmodel, train_dataset\u001b[38;5;241m=\u001b[39mtrain_dataset, tokenizer\u001b[38;5;241m=\u001b[39mtokenizer)\n\u001b[1;32m      8\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain(model, train_dataset)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Trainer' is not defined"
     ]
    }
   ],
   "source": [
    "# Train the model on your book features dataframe\n",
    "\n",
    "# create a dataset from `features_df` - about 80% of len(features_df)\n",
    "train_dataset = features_df.sample(1745)\n",
    "\n",
    "trainer = Trainer(model=model, train_dataset=train_dataset, tokenizer=tokenizer)\n",
    "\n",
    "trainer.train(model, train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe0816c-cb5f-40f9-bfde-31c97f404ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Step 3: Create an inference pipeline**\n",
    "\n",
    "def get_recommendations(user_input):\n",
    "    \"\"\"\n",
    "    Take in user input (e.g., favorite authors) and return recommended books.\n",
    "    \"\"\"\n",
    "    # Convert user input to a format the model can understand\n",
    "    user_features = ...  # e.g., tokenize text, create embeddings\n",
    "\n",
    "    # Use the trained model to generate recommendations\n",
    "    outputs = model(user_features)\n",
    "    predicted_books = [book for book in df.index if outputs[book] > threshold]\n",
    "\n",
    "    return predicted_books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27271656-5b9f-4945-827c-d28b9ee4c32e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd736de-922c-41dd-a1ff-e6df290ecf08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0ad2b1-d451-482a-8f02-d4268c2567f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf28ddd-b28a-40c0-9c47-17d474aa4cf4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
